{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News sources study Skripal (Section 4.5.1 of the thesis report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"../Engine/Engine.jl\")\n",
    "using .Engine\n",
    "\n",
    "using StatsBase, DataFrames, CSV\n",
    "using JSON, Dates\n",
    "using DataStructures\n",
    "import PyPlot as plt\n",
    "import Seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset(Skripal)\n",
    "df = skripal_dates(df)\n",
    "df = trust_score(df)\n",
    "f, _ = all_users()\n",
    "df = f(df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_before = df[df.partition .== \"Before campaign\", :]\n",
    "df_before = df_before[df_before.action .== \"U\", :]\n",
    "df_during = df[df.partition .== \"During campaign\", :]\n",
    "df_during = df_during[df_during.action .== \"U\", :]\n",
    "df_after = df[df.partition .== \"After campaign\", :]\n",
    "df_after = df_after[df_after.action .== \"U\", :]\n",
    "\n",
    "# 10 most influential users before Skripal according to TE\n",
    "users_TE_before = [\"peterpobjecky\", \"DontDenyThe\", \"JJorbyn\", \"wherepond\", \"shabbirh\", \"BeeAHoney_\", \"JuliaPolan\", \"PakamamaniRenew\", \"TheUrbanNewz\", \"londonfredd\"]\n",
    "df_TE_before = df_before[in.(df_before.username, Ref(users_TE_before)), :]\n",
    "\n",
    "# 10 most influential users during Skripal according to JDD\n",
    "users_JDD_during = [\"RT_com\", \"newsroll\", \"JJorbyn\", \"ferozwala\", \"paris_2015\", \"tonybrooklyn5\", \"QueensIceZ\", \"RLSRUSSIANNEWS\", \"dwilliam9940\", \"lisa_alba\"]\n",
    "df_JDD_during = df_during[in.(df_during.username, Ref(users_JDD_during)), :]\n",
    "# 10 most influential users during Skripal according to TE\n",
    "users_TE_during = [\"TheRealYoG\", \"ferozwala\", \"BuggerLePanda\", \"ProfessorsBlogg\", \"Arfatweet\", \"NecktopP\", \"OldRightie\", \"RTUKnews\", \"ali9l9\", \"zerohedge\"]\n",
    "df_TE_during = df_during[in.(df_during.username, Ref(users_TE_during)), :]\n",
    "\n",
    "# 10 most influential users after Skripal according to JDD\n",
    "users_JDD_after = [\"JudeJack\", \"HillestadNils\", \"SQUADDICTS\", \"starandsixpence\", \"newsbloktwit\", \"flyer4life\", \"jarfizo1\", \"DJSiri\", \"Mr_Nick_Nasty\", \"TacticalFM\"]\n",
    "df_JDD_after = df_after[in.(df_after.username, Ref(users_JDD_after)), :]\n",
    "# 10 most influential users after Skripal according to TE\n",
    "users_TE_after = [\"Pline999\", \"Revoche\", \"SnakeTera\", \"infidelchloe\", \"charlievictor16\", \"Char_lotte777\", \"LordGamblore\", \"iccjock06\", \"StephaniePetri1\", \"TheUrbanNewz\"]\n",
    "df_TE_after = df_after[in.(df_after.username, Ref(users_TE_after)), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change dataframe name to get data about a different partition (e.g. df_during -> df_before)\n",
    "domains = collect(Iterators.flatten(df_during.domain))\n",
    "count = countmap(domains)\n",
    "\n",
    "# Change dataframe name to get data about a different partition (e.g. df_JDD_during -> df_JDD_before)\n",
    "df_JDD = df_JDD_during\n",
    "domains_JDD = collect(Iterators.flatten(df_JDD.domain))\n",
    "count_JDD = countmap(domains_JDD)\n",
    "\n",
    "# Change dataframe name to get data about a different partition (e.g. df_TE_during -> df_TE_before)\n",
    "df_TE = df_TE_during\n",
    "domains_TE = collect(Iterators.flatten(df_TE.domain))\n",
    "count_TE = countmap(domains_TE)\n",
    "\n",
    "if !(length(unique(df_JDD.username)) == 10)\n",
    "    print(\"issue JDD\")\n",
    "end\n",
    "if !(length(unique(df_TE.username)) == 10)\n",
    "    print(\"issue TE\")\n",
    "end\n",
    "\n",
    "println(\"rt :\")\n",
    "println(\"All : $((\"rt.com\" in keys(count) ? count[\"rt.com\"] : 0) / length(unique(df_during.username)))\")\n",
    "println(\"JDD : $((\"rt.com\" in keys(count_JDD) ? count_JDD[\"rt.com\"] : 0) / length(unique(df_JDD.username)))\")\n",
    "println(\"TE : $((\"rt.com\" in keys(count_TE) ? count_TE[\"rt.com\"] : 0)/ length(unique(df_TE.username)))\")\n",
    "\n",
    "println(\"\")\n",
    "println(\"sputnik :\")\n",
    "println(\"All : $((\"sputniknews.com\" in keys(count) ? count[\"sputniknews.com\"] : 0)/ length(unique(df_during.username)))\")\n",
    "println(\"JDD : $((\"sputniknews.com\" in keys(count_JDD) ? count_JDD[\"sputniknews.com\"] : 0) / length(unique(df_JDD.username)))\")\n",
    "println(\"TE : $((\"sputniknews.com\" in keys(count_TE) ? count_TE[\"sputniknews.com\"] : 0) / length(unique(df_TE.username)))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News sources study COP26 (Section 4.5.2 of the thesis report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"../Engine/Engine.jl\")\n",
    "using .Engine\n",
    "\n",
    "using StatsBase, DataFrames, CSV\n",
    "using JSON, Dates\n",
    "using DataStructures\n",
    "import PyPlot as plt\n",
    "import Seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset(COP26)\n",
    "df = cop_26_dates(df)\n",
    "df = trust_score(df)\n",
    "f, _ = all_users()\n",
    "df = f(df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_during = df[df.partition .== \"During COP26\", :]\n",
    "df_during = df_during[df_during.action .== \"U\", :]\n",
    "\n",
    "\n",
    "# 5 most influential users during COP26 according to JDD\n",
    "users_JDD_during = [\"globaltimesnews\", \"MSNBC\", \"PepperInVegas\", \"CGTNOfficial\", \"Chris_1791\"]\n",
    "df_JDD_during = df_during[in.(df_during.username, Ref(users_JDD_during)), :]\n",
    "\n",
    "# 5 most influential users during COP26 according to TE\n",
    "users_TE_during = [\"delmartian4\", \"AdoreUSAalways\", \"JJDJ1187\", \"TheRebeluniter\", \"TimMelino\"]\n",
    "df_TE_during = df_during[in.(df_during.username, Ref(users_TE_during)), :];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafolder = PROJECT_FOLDER * \"/Data/Twitter/COP26_processed\"\n",
    "files = [file for file in readdir(datafolder, join=true) if occursin(\".json\", file)]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Remap tweets to the full URLs contained in them (the dataframes only contain the domains).\n",
    "\"\"\"\n",
    "function remap(files, df1, df2)\n",
    "\n",
    "    to_datetime = x -> DateTime(split(x, '.')[1], \"yyyy-mm-ddTHH:MM:SS\")\n",
    "    df1_full_urls = [[] for i = 1:length(df1[!,1])]\n",
    "    df2_full_urls = [[] for i = 1:length(df2[!,1])]\n",
    "\n",
    "    for file in files\n",
    "\n",
    "        for line in eachline(file)\n",
    "\n",
    "            dic = JSON.parse(line, null=missing)\n",
    "            dic[\"created_at\"] = to_datetime(dic[\"created_at\"])\n",
    "\n",
    "            index1 = findfirst(dic[\"created_at\"] .== df1.created_at)\n",
    "            if !isnothing(index1)\n",
    "                if dic[\"username\"] == df1[index1, \"username\"] && dic[\"domain\"] == df1[index1, \"domain\"] && dic[\"sentiment\"] == df1[index1, \"sentiment\"] \n",
    "                    df1_full_urls[index1] = dic[\"urls\"]\n",
    "                end\n",
    "            end\n",
    "\n",
    "            index2 = findfirst(dic[\"created_at\"] .== df2.created_at)\n",
    "            if !isnothing(index2)\n",
    "                if dic[\"username\"] == df2[index2, \"username\"] && dic[\"domain\"] == df2[index2, \"domain\"] && dic[\"sentiment\"] == df2[index2, \"sentiment\"] \n",
    "                    df2_full_urls[index2] = dic[\"urls\"]\n",
    "                end\n",
    "            end\n",
    "\n",
    "        end\n",
    "\n",
    "    end\n",
    "                    \n",
    "    df1.urls = df1_full_urls\n",
    "    df2.urls = df2_full_urls\n",
    "\n",
    "    return df1, df2\n",
    "\n",
    "end\n",
    "\n",
    "\n",
    "df_JDD_during, df_TE_during = remap(files, df_JDD_during, df_TE_during);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = CSV.read(PreProcessing.FULL_NEWSGUARD_TABLE, DataFrame, header=1)\n",
    "\n",
    "\"\"\"\n",
    "Find the URLs and domaisn that were used to derive the graphs (the first match found for tweets having more than one URL).\n",
    "\"\"\"\n",
    "function effective_domain_and_urls(df, news_outlet::DataFrame)\n",
    "\n",
    "    # Initialize vectors of strings\n",
    "    effective_domain = String[\"0\" for i = 1:length(df[!,1])]\n",
    "    effective_url = String[\"0\" for i = 1:length(df[!,1])]\n",
    "    \n",
    "    for (i, domains) in enumerate(df.domain)\n",
    "\n",
    "        for (j, domain) in enumerate(domains)\n",
    "            index = findfirst(domain .== news_outlet.\"domain\")\n",
    "            if !isnothing(index)\n",
    "                effective_domain[i] = domain\n",
    "                effective_url[i] = df.urls[i][j]\n",
    "            end\n",
    "        end\n",
    "\n",
    "\tend\n",
    "\n",
    "    df.effective_domain = effective_domain\n",
    "    df.effective_url = effective_url\n",
    "\n",
    "    return df\n",
    "\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Find the URLs that were used to derive the graphs (the first match found for tweets having more than one URL).\n",
    "\"\"\"\n",
    "function effective_domain(df, news_outlet::DataFrame)\n",
    "\n",
    "    effective_domain = String[\"0\" for i = 1:length(df[!,1])]\n",
    "    \n",
    "    for (i, domains) in enumerate(df.domain)\n",
    "\n",
    "        for (j, domain) in enumerate(domains)\n",
    "            index = findfirst(domain .== news_outlet.\"domain\")\n",
    "            if !isnothing(index)\n",
    "                effective_domain[i] = domain\n",
    "            end\n",
    "        end\n",
    "\n",
    "\tend\n",
    "\n",
    "    df.effective_domain = effective_domain\n",
    "\n",
    "    return df\n",
    "\n",
    "end\n",
    "\n",
    "df_during = effective_domain(df_during, news)\n",
    "df_JDD_during = effective_domain_and_urls(df_JDD_during, news)\n",
    "df_TE_during = effective_domain_and_urls(df_TE_during, news);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains_JDD = df_JDD_during.effective_domain\n",
    "domains_TE = df_TE_during.effective_domain\n",
    "\n",
    "countmap_JDD = countmap(domains_JDD)\n",
    "countmap_TE = countmap(domains_TE)\n",
    "\n",
    "\n",
    "urls_JDD = df_JDD_during.effective_url\n",
    "urls_TE = df_TE_during.effective_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = OrderedDict(\n",
    "    \"msnbc.com\" => \"tab:green\",\n",
    "    \"thegatewaypundit.com\" => \"tab:orange\",\n",
    "    \"zerohedge.com\" => \"tab:purple\",\n",
    "    \"foxnews.com\" => \"tab:green\",\n",
    "    \"breitbart.com\" => \"tab:orange\",\n",
    "    \"globaltimes.cn\" => \"tab:red\",\n",
    "    \"cgtn.com\" => \"tab:red\",\n",
    "    \"beckernews.com\" => \"tab:green\"\n",
    ")\n",
    "\n",
    "politicmap = OrderedDict(\n",
    "    \"tab:red\" => \"China affiliated\",\n",
    "    \"tab:orange\" => \"US far right\",\n",
    "    \"tab:purple\" => \"far right\",\n",
    "    \"tab:green\" => \"other\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_JDD = [colormap[domain] for domain in keys(countmap_JDD)]\n",
    "legend_elements = [plt.matplotlib.patches.Patch(facecolor=color, label=politicmap[color]) for color in keys(politicmap) if color in unique(colors_JDD)]\n",
    "\n",
    "\n",
    "indices = sortperm(collect(values(countmap_JDD)), rev=false)\n",
    "vals = collect(values(countmap_JDD))[indices]\n",
    "names = collect(keys(countmap_JDD))[indices]\n",
    "colors_JDD = colors_JDD[indices]\n",
    "\n",
    "plt.figure()\n",
    "plt.barh(1:length(countmap_JDD), vals, color=colors_JDD)\n",
    "plt.xlabel(\"Number of tweets\")\n",
    "plt.ylabel(\"News sources\")\n",
    "plt.legend(handles=legend_elements, loc=\"best\")\n",
    "plt.yticks(1:length(countmap_JDD), names)\n",
    "lims = plt.xlim()\n",
    "# plt.savefig(PROJECT_FOLDER * \"/Figures/news_source_study/JDD_COP_26_during.pdf\", bbox_inches=\"tight\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_TE = [colormap[domain] for domain in keys(countmap_TE)]\n",
    "legend_elements = [plt.matplotlib.patches.Patch(facecolor=color, label=politicmap[color]) for color in keys(politicmap) if color in unique(colors_TE)]\n",
    "\n",
    "\n",
    "indices = sortperm(collect(values(countmap_TE)), rev=false)\n",
    "vals = collect(values(countmap_TE))[indices]\n",
    "names = collect(keys(countmap_TE))[indices]\n",
    "colors_TE = colors_TE[indices]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6.4, 4.8*4/7))\n",
    "plt.barh(1:length(countmap_TE), vals, color=colors_TE)\n",
    "plt.xlabel(\"Number of tweets\")\n",
    "plt.ylabel(\"News sources\")\n",
    "plt.legend(handles=legend_elements, loc=\"best\")\n",
    "plt.yticks(1:length(countmap_TE), names)\n",
    "plt.xlim(lims)\n",
    "# plt.savefig(PROJECT_FOLDER * \"/Figures/news_source_study/TE_COP_26_during.pdf\", bbox_inches=\"tight\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_urls = unique(urls_TE)\n",
    "\n",
    "print(unique_urls[occursin.(\"thegatewaypundit\", unique_urls)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
