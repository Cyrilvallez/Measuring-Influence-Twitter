{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"../Engine/Engine.jl\")\n",
    "using .Engine\n",
    "\n",
    "using StatsBase, DataFrames\n",
    "import PyPlot as plt\n",
    "import Seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>446,408 rows × 6 columns (omitted printing of 1 columns)</p><table class=\"data-frame\"><thead><tr><th></th><th>created_at</th><th>effective_category</th><th>domain</th><th>username</th><th>follower_count</th></tr><tr><th></th><th title=\"Dates.DateTime\">DateTime</th><th title=\"String\">String</th><th title=\"Vector{Any}\">Array…</th><th title=\"String\">String</th><th title=\"Int64\">Int64</th></tr></thead><tbody><tr><th>1</th><td>2018-03-06T23:59:28</td><td>retweet</td><td>[&quot;theguardian.com&quot;]</td><td>Tesscatbird</td><td>979</td></tr><tr><th>2</th><td>2018-03-06T23:59:01</td><td>retweet</td><td>[&quot;theguardian.com&quot;]</td><td>crlulukat</td><td>440</td></tr><tr><th>3</th><td>2018-03-06T23:58:36</td><td>retweet</td><td>[&quot;nbcnews.com&quot;]</td><td>tiffanyclay</td><td>2909</td></tr><tr><th>4</th><td>2018-03-06T23:58:22</td><td>retweet</td><td>[&quot;theguardian.com&quot;]</td><td>KarCranky</td><td>242</td></tr><tr><th>5</th><td>2018-03-06T23:58:22</td><td>retweet</td><td>[&quot;theguardian.com&quot;]</td><td>electricboyo</td><td>1573</td></tr><tr><th>6</th><td>2018-03-06T23:58:11</td><td>retweet</td><td>[&quot;nbcnews.com&quot;]</td><td>rhscsmtms</td><td>1420</td></tr><tr><th>7</th><td>2018-03-06T23:57:59</td><td>retweet</td><td>[&quot;nbcnews.com&quot;]</td><td>AJ_9342</td><td>138</td></tr><tr><th>8</th><td>2018-03-06T23:57:58</td><td>retweet</td><td>[&quot;theguardian.com&quot;]</td><td>unitedMongrels</td><td>1303</td></tr><tr><th>9</th><td>2018-03-06T23:57:34</td><td>retweet</td><td>[&quot;nbcnews.com&quot;]</td><td>SouthFlorida42</td><td>1721</td></tr><tr><th>10</th><td>2018-03-06T23:57:16</td><td>retweet</td><td>[&quot;nbcnews.com&quot;]</td><td>RevFrBobHutchis</td><td>627</td></tr><tr><th>11</th><td>2018-03-06T23:57:13</td><td>retweet</td><td>[&quot;theguardian.com&quot;]</td><td>PlayaSharon</td><td>1598</td></tr><tr><th>12</th><td>2018-03-06T23:57:02</td><td>retweet</td><td>[&quot;nbcnews.com&quot;]</td><td>lizditz</td><td>8414</td></tr><tr><th>13</th><td>2018-03-06T23:57:02</td><td>tweet</td><td>[&quot;theguardian.com&quot;]</td><td>andreariscassi</td><td>15577</td></tr><tr><th>14</th><td>2018-03-06T23:57:01</td><td>retweet</td><td>[&quot;nbcnews.com&quot;]</td><td>Cuprikorn66</td><td>1583</td></tr><tr><th>15</th><td>2018-03-06T23:56:52</td><td>retweet</td><td>[&quot;nbcnews.com&quot;]</td><td>NotJAFO</td><td>834</td></tr><tr><th>16</th><td>2018-03-06T23:56:51</td><td>retweet</td><td>[&quot;theguardian.com&quot;]</td><td>CeleryCaraway</td><td>1254</td></tr><tr><th>17</th><td>2018-03-06T23:56:47</td><td>retweet</td><td>[&quot;theguardian.com&quot;]</td><td>merrill2n</td><td>485</td></tr><tr><th>18</th><td>2018-03-06T23:56:42</td><td>retweet</td><td>[&quot;nbcnews.com&quot;]</td><td>Federelis</td><td>407</td></tr><tr><th>19</th><td>2018-03-06T23:56:24</td><td>retweet</td><td>[&quot;nbcnews.com&quot;]</td><td>Ceo_Leo</td><td>218</td></tr><tr><th>20</th><td>2018-03-06T23:56:23</td><td>retweet</td><td>[&quot;nbcnews.com&quot;]</td><td>fwdcrocblu</td><td>1878</td></tr><tr><th>21</th><td>2018-03-06T23:56:23</td><td>tweet</td><td>[&quot;theguardian.com&quot;]</td><td>the_Dyp</td><td>1430</td></tr><tr><th>22</th><td>2018-03-06T23:56:20</td><td>retweet</td><td>[&quot;theguardian.com&quot;]</td><td>5dodgerfans</td><td>137</td></tr><tr><th>23</th><td>2018-03-06T23:56:16</td><td>retweet</td><td>[&quot;theguardian.com&quot;]</td><td>LenteCurrite</td><td>489</td></tr><tr><th>24</th><td>2018-03-06T23:56:11</td><td>retweet</td><td>[&quot;theguardian.com&quot;]</td><td>LenteCurrite</td><td>489</td></tr><tr><th>25</th><td>2018-03-06T23:56:10</td><td>retweet</td><td>[&quot;theguardian.com&quot;]</td><td>Despisingdespot</td><td>139</td></tr><tr><th>26</th><td>2018-03-06T23:56:02</td><td>retweet</td><td>[&quot;nbcnews.com&quot;]</td><td>jo_revelette</td><td>1242</td></tr><tr><th>27</th><td>2018-03-06T23:56:00</td><td>retweet</td><td>[&quot;nbcnews.com&quot;]</td><td>setislady</td><td>3641</td></tr><tr><th>28</th><td>2018-03-06T23:55:52</td><td>retweet</td><td>[&quot;nbcnews.com&quot;]</td><td>Russbarr</td><td>7155</td></tr><tr><th>29</th><td>2018-03-06T23:55:51</td><td>retweet</td><td>[&quot;nbcnews.com&quot;]</td><td>PumaBare</td><td>4580</td></tr><tr><th>30</th><td>2018-03-06T23:55:50</td><td>retweet</td><td>[&quot;theguardian.com&quot;]</td><td>lroy2737</td><td>136</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccc}\n",
       "\t& created\\_at & effective\\_category & domain & username & follower\\_count & \\\\\n",
       "\t\\hline\n",
       "\t& DateTime & String & Array… & String & Int64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 2018-03-06T23:59:28 & retweet & [\"theguardian.com\"] & Tesscatbird & 979 & $\\dots$ \\\\\n",
       "\t2 & 2018-03-06T23:59:01 & retweet & [\"theguardian.com\"] & crlulukat & 440 & $\\dots$ \\\\\n",
       "\t3 & 2018-03-06T23:58:36 & retweet & [\"nbcnews.com\"] & tiffanyclay & 2909 & $\\dots$ \\\\\n",
       "\t4 & 2018-03-06T23:58:22 & retweet & [\"theguardian.com\"] & KarCranky & 242 & $\\dots$ \\\\\n",
       "\t5 & 2018-03-06T23:58:22 & retweet & [\"theguardian.com\"] & electricboyo & 1573 & $\\dots$ \\\\\n",
       "\t6 & 2018-03-06T23:58:11 & retweet & [\"nbcnews.com\"] & rhscsmtms & 1420 & $\\dots$ \\\\\n",
       "\t7 & 2018-03-06T23:57:59 & retweet & [\"nbcnews.com\"] & AJ\\_9342 & 138 & $\\dots$ \\\\\n",
       "\t8 & 2018-03-06T23:57:58 & retweet & [\"theguardian.com\"] & unitedMongrels & 1303 & $\\dots$ \\\\\n",
       "\t9 & 2018-03-06T23:57:34 & retweet & [\"nbcnews.com\"] & SouthFlorida42 & 1721 & $\\dots$ \\\\\n",
       "\t10 & 2018-03-06T23:57:16 & retweet & [\"nbcnews.com\"] & RevFrBobHutchis & 627 & $\\dots$ \\\\\n",
       "\t11 & 2018-03-06T23:57:13 & retweet & [\"theguardian.com\"] & PlayaSharon & 1598 & $\\dots$ \\\\\n",
       "\t12 & 2018-03-06T23:57:02 & retweet & [\"nbcnews.com\"] & lizditz & 8414 & $\\dots$ \\\\\n",
       "\t13 & 2018-03-06T23:57:02 & tweet & [\"theguardian.com\"] & andreariscassi & 15577 & $\\dots$ \\\\\n",
       "\t14 & 2018-03-06T23:57:01 & retweet & [\"nbcnews.com\"] & Cuprikorn66 & 1583 & $\\dots$ \\\\\n",
       "\t15 & 2018-03-06T23:56:52 & retweet & [\"nbcnews.com\"] & NotJAFO & 834 & $\\dots$ \\\\\n",
       "\t16 & 2018-03-06T23:56:51 & retweet & [\"theguardian.com\"] & CeleryCaraway & 1254 & $\\dots$ \\\\\n",
       "\t17 & 2018-03-06T23:56:47 & retweet & [\"theguardian.com\"] & merrill2n & 485 & $\\dots$ \\\\\n",
       "\t18 & 2018-03-06T23:56:42 & retweet & [\"nbcnews.com\"] & Federelis & 407 & $\\dots$ \\\\\n",
       "\t19 & 2018-03-06T23:56:24 & retweet & [\"nbcnews.com\"] & Ceo\\_Leo & 218 & $\\dots$ \\\\\n",
       "\t20 & 2018-03-06T23:56:23 & retweet & [\"nbcnews.com\"] & fwdcrocblu & 1878 & $\\dots$ \\\\\n",
       "\t21 & 2018-03-06T23:56:23 & tweet & [\"theguardian.com\"] & the\\_Dyp & 1430 & $\\dots$ \\\\\n",
       "\t22 & 2018-03-06T23:56:20 & retweet & [\"theguardian.com\"] & 5dodgerfans & 137 & $\\dots$ \\\\\n",
       "\t23 & 2018-03-06T23:56:16 & retweet & [\"theguardian.com\"] & LenteCurrite & 489 & $\\dots$ \\\\\n",
       "\t24 & 2018-03-06T23:56:11 & retweet & [\"theguardian.com\"] & LenteCurrite & 489 & $\\dots$ \\\\\n",
       "\t25 & 2018-03-06T23:56:10 & retweet & [\"theguardian.com\"] & Despisingdespot & 139 & $\\dots$ \\\\\n",
       "\t26 & 2018-03-06T23:56:02 & retweet & [\"nbcnews.com\"] & jo\\_revelette & 1242 & $\\dots$ \\\\\n",
       "\t27 & 2018-03-06T23:56:00 & retweet & [\"nbcnews.com\"] & setislady & 3641 & $\\dots$ \\\\\n",
       "\t28 & 2018-03-06T23:55:52 & retweet & [\"nbcnews.com\"] & Russbarr & 7155 & $\\dots$ \\\\\n",
       "\t29 & 2018-03-06T23:55:51 & retweet & [\"nbcnews.com\"] & PumaBare & 4580 & $\\dots$ \\\\\n",
       "\t30 & 2018-03-06T23:55:50 & retweet & [\"theguardian.com\"] & lroy2737 & 136 & $\\dots$ \\\\\n",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ArgParse, StatsBase\n",
    "import Random\n",
    "Random.seed!(12)\n",
    "# import PyPlot as plt\n",
    "# using PyPlot: @L_str\n",
    "# import Seaborn as sns\n",
    "\n",
    "include(\"Engine/Engine.jl\")\n",
    "using .Engine\n",
    "\n",
    "function draw_series(N, M = 150, min_ = 3, weights = nothing)\n",
    "    if isnothing(weights)\n",
    "        weights = AnalyticWeights([0.965, 0.03, 0.003, 0.001, 0.001])\n",
    "    end\n",
    "\n",
    "    sample_ = [sample(0:4, weights, M) for i = 1:N]\n",
    "    for i = 1:N\n",
    "        if sum(sample_[i]) < min_\n",
    "            while sum(sample_[i]) < min_\n",
    "                sample_[i] = sample(0:4, weights, M)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return sample_\n",
    "\n",
    "end\n",
    "\n",
    "\n",
    "args = parse_commandline()\n",
    "\n",
    "\n",
    "N 10\n",
    "N_redo_all = 5\n",
    "N_redo_surro = 5\n",
    "\n",
    "# Random seeds\n",
    "seeds_all = sample(1:10000, N_redo_all, replace=false)\n",
    "seeds_surro = sample(1:10000, N_redo_surro, replace=false)\n",
    "\n",
    "\n",
    "for (k, seed_all) in ProgressBar(enumerate(seeds_all), \"All JDD\", leave=true)\n",
    "\n",
    "    Random.seed!(seed_all)\n",
    "    X = draw_series(N)\n",
    "    Y = draw_series(N)\n",
    "\n",
    "    for (l, seed_surro) in ProgressBar(enumerate(seeds_surro), \"Seeds\", leave=false)\n",
    "\n",
    "        for (i, limit) in ProgressBar(enumerate(limits2), \"Limits\", leave=false)\n",
    "            for (j, threshold) in ProgressBar(enumerate(thresholds2), \"Thresholds\", leave=false)\n",
    "\n",
    "                Random.seed!(seed_surro)\n",
    "                igg = InfluenceGraphGenerator(JointDistanceDistribution, threshold=threshold, limit=limit)\n",
    "                tot = 0\n",
    "\n",
    "                for idx in ProgressBar(1:N, leave=false)\n",
    "                    x = Sensors.standardize(X[idx])\n",
    "                    y = Sensors.standardize(Y[idx])\n",
    "                    if igg.causal_function(x, y) == 1\n",
    "                        tot += 1\n",
    "                    end  \n",
    "                end\n",
    "\n",
    "                result2[i,j][k,l] = tot/N\n",
    "\n",
    "            end\n",
    "        end\n",
    "\n",
    "    end\n",
    "\n",
    "end\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"/Users/cyrilvallez/Desktop/Thesis/Data/Twitter/Skripal_processed_lightweight\"\n",
    "files = [file for file in readdir(folder, join=true) if occursin(\".json\", file)]\n",
    "\n",
    "df1 = vcat([Helpers.load_json(file) for file in files]...)\n",
    "df2 = Helpers.load_json(\"/Users/cyrilvallez/Desktop/Thesis/Data/BrandWatch/Skripal/skripal_clean_lightweight.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(df1.created_at, bins=100)\n",
    "xloc, xlabels = plt.xticks()\n",
    "plt.xticks(xloc, xlabels, rotation=\"vertical\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(df2.created_at, bins=100)\n",
    "xloc, xlabels = plt.xticks()\n",
    "plt.xticks(xloc, xlabels, rotation=\"vertical\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = load_dataset(Skripal)\n",
    "df3 = skripal_dates(df3)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(df3.created_at, bins=100)\n",
    "xloc, xlabels = plt.xticks()\n",
    "plt.xticks(xloc, xlabels, rotation=\"vertical\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.9*0.15^i for i = 0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using StatsBase\n",
    "\n",
    "weights = AnalyticWeights([0.965, 0.03, 0.003, 0.001, 0.001])\n",
    "b = AnalyticWeights(weights)\n",
    "\n",
    "foo = [sample(0:4, b, 150) for i = 1:10000]\n",
    "test = [sum(a) for a in foo]\n",
    "\n",
    "plt.figure()\n",
    "plt.boxplot(test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function draw_series(N, M = 150, weights = nothing)\n",
    "    if isnothing(weights)\n",
    "        weights = AnalyticWeights([0.965, 0.03, 0.003, 0.001, 0.001])\n",
    "    end\n",
    "\n",
    "    sample_ = [sample(0:4, weights, M) for i = 1:N]\n",
    "    for i = 1:N\n",
    "        if sum(sample_[i]) < 3\n",
    "            while sum(sample_[i]) < 3\n",
    "                sample_[i] = sample(0:4, weights, M)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return sample_\n",
    "\n",
    "end\n",
    "\n",
    "foo = draw_series(10000)\n",
    "test = [sum(a) for a in foo]\n",
    "\n",
    "plt.figure()\n",
    "plt.boxplot(test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10000*100*2.5e-3/3600*25*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset(Skripal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset(Skripal)\n",
    "df = skripal_dates(df)\n",
    "df = trust_score(df)\n",
    "f, _ = all_users()\n",
    "df = f(df)\n",
    "\n",
    "plot_action_frequency(df, save=true, log=false, filename=\"../../Figures/Datasets/Skripal_all_users/action_frequency.pdf\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = skripal_dates(df)\n",
    "f, _ = all_users()\n",
    "df = f(df)\n",
    "\n",
    "combine(groupby(df, \"partition\"), \"username\" => (x->length(unique(x))) => \"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Seaborn as sns\n",
    "\n",
    "result = load_data(\"/Users/cyrilvallez/Desktop/Thesis/Results/Find_thresholds/new_test2_TE.jld2\")\n",
    "labels = [\"None\", \"Q(0.5)\", \"Q(0.75)\", \"Q(0.9)\", \"max\", \"2max\", \"4max\"]\n",
    "thresholds = 0:0.1:0.6\n",
    "\n",
    "\n",
    "mean_value = Matrix{Float64}(undef, size(result))\n",
    "for i in eachindex(result)\n",
    "    mean_value[i] = mean(result[i])\n",
    "end\n",
    "\n",
    "if any(mean_value .== 0)\n",
    "    vmin = minimum(mean_value[mean_value .!= 0])*0.5\n",
    "else\n",
    "    vmin = minimum(mean_value)\n",
    "end\n",
    "\n",
    "plt.figure(figsize=[6.4, 4.8].*1.2)\n",
    "sns.heatmap(mean_value, annot=annot, cmap=\"rocket_r\", fmt=\"\", norm=plt.matplotlib.colors.LogNorm(vmin=vmin, clip=true))\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Limit value\")\n",
    "xloc, xlabels = plt.xticks()\n",
    "plt.xticks(xloc, thresholds)\n",
    "yloc, ylabels = plt.yticks()\n",
    "plt.yticks(yloc, labels, rotation=\"horizontal\")\n",
    "plt.gcf();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Seaborn as sns\n",
    "\n",
    "result = load_data(\"/Users/cyrilvallez/Desktop/Thesis/Results/Find_thresholds/new_test2_TE.jld2\")\n",
    "labels = [\"None\", \"Q(0.5)\", \"Q(0.75)\", \"Q(0.9)\", \"max\", \"2max\", \"4max\"]\n",
    "thresholds = 0:0.1:0.6\n",
    "\n",
    "\n",
    "mean_value = Matrix{Float64}(undef, size(result))\n",
    "for i in eachindex(result)\n",
    "    mean_value[i] = mean(result[i])\n",
    "end\n",
    "\n",
    "if any(mean_value .== 0)\n",
    "    vmin = minimum(mean_value[mean_value .!= 0])*0.5\n",
    "else\n",
    "    vmin = minimum(mean_value)\n",
    "end\n",
    "\n",
    "plt.figure(figsize=[6.4, 4.8].*1.2)\n",
    "sns.heatmap(mean_value, annot=true, cmap=\"rocket_r\", fmt=\".2g\", norm=plt.matplotlib.colors.LogNorm(vmin=vmin, clip=true))\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Limit value\")\n",
    "xloc, xlabels = plt.xticks()\n",
    "plt.xticks(xloc, thresholds)\n",
    "yloc, ylabels = plt.yticks()\n",
    "plt.yticks(yloc, labels, rotation=\"horizontal\")\n",
    "plt.gcf();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cop26 = load_dataset(COP26)\n",
    "ranks = get_all_ranks(cop26, cop_26_dates)\n",
    "\n",
    "graphs, cascades, df = load_data(\"/home/ubuntu/Thesis/Results/JDD_all_users/COP26/data.jld2\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# sns.heatmap(mean_value, annot=true, cmap=\"rocket_r\")\n",
    "# plt.xlabel(\"Threshold\")\n",
    "# plt.ylabel(\"Limit value\")\n",
    "# xloc, xlabels = plt.xticks()\n",
    "# plt.xticks(xloc, thresholds)\n",
    "# yloc, ylabels = plt.yticks()\n",
    "# plt.yticks(yloc, labels, rotation=\"horizontal\")\n",
    "# plt.savefig(path * \"_TE.pdf\", bbox_inches=\"tight\")\n",
    "# plt.gcf()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks[end-20:end, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Graphs, SimpleWeightedGraphs\n",
    "edge_type = \"Any Edge\"\n",
    "N_biggest = 20\n",
    "cuttoff = 0\n",
    "\n",
    "between, between_actors = betweenness_centralities2(graphs, df, cuttoff=cuttoff, edge_type=edge_type)\n",
    "in_, in_actors = indegree_centralities2(graphs, df, cuttoff=cuttoff, edge_type=edge_type)\n",
    "out, out_actors = outdegree_centralities2(graphs, df, cuttoff=cuttoff, edge_type=edge_type)\n",
    "\n",
    "between_actors[3][1:N_biggest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_actors[3][1:N_biggest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranks.tweet_rank = ordinalrank(ranks.tweet_count, rev=true)\n",
    "foo = ranks[ranks.partition .== \"During COP26\", [\"username\", \"tweet_rank\", \"tweet_count\"]]\n",
    "sort!(foo, :tweet_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = \"/home/ubuntu/Thesis/Results/JDD_all_users/COP26-After_COP26/data.jld2\"\n",
    "file2 = \"/home/ubuntu/Thesis/Results/JDD_all_users/COP26-Before_COP26/data.jld2\"\n",
    "file3 = \"/home/ubuntu/Thesis/Results/JDD_all_users/COP26-During_COP26/data.jld2\"\n",
    "\n",
    "df = load_dataset(COP26)\n",
    "\n",
    "# Define partitions, actions and actors\n",
    "partitions = cop_26_dates\n",
    "actions = trust_score\n",
    "actors = all_users(by_partition=true, min_tweets=3)\n",
    "agents = PreProcessingAgents(partitions, actions, actors)\n",
    "\n",
    "df = preprocessing(df, agents)\n",
    "\n",
    "graph1, cascade1, _ = load_data(file1)\n",
    "graph2, cascade2, _ = load_data(file2)\n",
    "graph3, cascade3, _ = load_data(file3)\n",
    "\n",
    "graph1 = graph1[1]\n",
    "cascade1 = cascade1[1]\n",
    "graph2 = graph2[1]\n",
    "cascade2 = cascade2[1]\n",
    "graph3 = graph3[1]\n",
    "cascade3 = cascade3[1]\n",
    "\n",
    "graphs = [graph1, graph2, graph3]\n",
    "cascades = [cascade1, cascade2, cascade3]\n",
    "\n",
    "# mkpath(\"/home/ubuntu/Thesis/Results/JDD_all_users/COP26\")\n",
    "# save_data(graphs, cascades, df, \"/home/ubuntu/Thesis/Results/JDD_all_users/COP26/data.jld2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename1 = \"/Users/cyrilvallez/Desktop/Thesis/Data/Twitter/Skripal_processed_lightweight\"\n",
    "filename2 = \"/Users/cyrilvallez/Desktop/Thesis/Data/BrandWatch/Skripal/skripal_clean_lightweight.json\"\n",
    "\n",
    "datafiles1 = [file for file in readdir(filename1, join=true) if occursin(\".json\", file)]\n",
    "\n",
    "df1 = vcat([Helpers.load_json(file) for file in datafiles1]...)\n",
    "df3 = Helpers.load_json(filename2)\n",
    "\n",
    "df1 = df1[:, Not(\"sentiment\")]\n",
    "\n",
    "sum_ = 0\n",
    "for i = 1:length(df1[:, 1])\n",
    "    if df1[1,:] in eachrow(df3)\n",
    "        sum_ += 1\n",
    "    end\n",
    "end\n",
    "sum_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Languages\n",
    "import CSV\n",
    "using DataFrames\n",
    "\n",
    "detector = LanguageDetector()\n",
    "\n",
    "df2 = Helpers.load_json(\"/Users/cyrilvallez/Desktop/Thesis/Data/BrandWatch/Skripal/skripal_clean.json\")\n",
    "news = CSV.read(\"/Users/cyrilvallez/Desktop/Thesis/Data/newsguard_full_table_clean.csv\", DataFrame)\n",
    "\n",
    "isin = (x,y) -> ismissing(x) ? missing : any([any(elem .== y) for elem in x])\n",
    "mask = isin.(df2.domain, Ref(news.domain))\n",
    "df2 = df2[mask, :]\n",
    "\n",
    "df2.lang = detector.(df2.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using StatsBase\n",
    "include(\"../Engine/Engine.jl\")\n",
    "using .Engine\n",
    "import PyPlot as plt\n",
    "import Seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset(Skripal)\n",
    "df = skripal_dates(df);\n",
    "df = trust_score(df);\n",
    "f, _ = IP_scores(by_partition=false)\n",
    "df = f(df);\n",
    "\n",
    "x = df.\"username\"\n",
    "indices = unique(i -> x[i], 1:length(x))\n",
    "# Get unique usernames and corresponding follower_count\n",
    "users = x[indices]\n",
    "I = df.I_score[indices]\n",
    "P = df.P_score[indices]\n",
    "\n",
    "sorting = sortperm(I, rev=true)\n",
    "users = users[sorting]\n",
    "P = P[sorting]\n",
    "I = I[sorting]\n",
    "\n",
    "print(users[1:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/Users/cyrilvallez/Desktop/Thesis/Data/Twitter/Skripal_processed_lightweight\"\n",
    "files = [file for file in readdir(filename, join=true)]\n",
    "frames = [Helpers.load_json(file) for file in files]\n",
    "df = vcat(frames...)\n",
    "\n",
    "\n",
    "df = skripal_dates(df);\n",
    "df = trust_score(df);\n",
    "f, _ = IP_scores(by_partition=false)\n",
    "df = f(df);\n",
    "\n",
    "x = df.\"username\"\n",
    "indices = unique(i -> x[i], 1:length(x))\n",
    "# Get unique usernames and corresponding follower_count\n",
    "users = x[indices]\n",
    "I = df.I_score[indices]\n",
    "P = df.P_score[indices]\n",
    "\n",
    "sorting = sortperm(I, rev=true)\n",
    "users = users[sorting]\n",
    "P = P[sorting]\n",
    "I = I[sorting]\n",
    "\n",
    "print(users[1:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(users[1:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = load_dataset(COP26)\n",
    "df2 = cop_26_dates(df2)\n",
    "f, _ = IP_scores(actor_number=\"all_positive\", aggregate_size=100000000)\n",
    "df2 = f(df2)\n",
    "\n",
    "combine(groupby(df2, \"partition\"), \"actor\" => (x->length(unique(x))) => \"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = load_dataset(COP26)\n",
    "df2 = cop_26_dates(df2)\n",
    "f, _ = IP_scores(actor_number=\"all\", aggregate_size=100000000)\n",
    "df2 = f(df2)\n",
    "\n",
    "combine(groupby(df2, \"partition\"), \"actor\" => (x->length(unique(x))) => \"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.\"username\"\n",
    "indices = unique(i -> x[i], 1:length(x))\n",
    "# Get unique usernames and corresponding follower_count\n",
    "users = x[indices]\n",
    "I = df.I_score[indices]\n",
    "P = df.P_score[indices]\n",
    "\n",
    "sorting = sortperm(I, rev=true)\n",
    "users = users[sorting]\n",
    "P = P[sorting]\n",
    "I = I[sorting]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rand(200)\n",
    "y = rand(200)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df2.I_score .> 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPlot as plt\n",
    "\n",
    "plt.hist(df2.created_at, bins=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_action_frequency(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rand(0:1, 200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CausalityTools\n",
    "import Random\n",
    "Random.seed!(12)\n",
    "\n",
    "surros = []\n",
    "generator = surrogenerator(x, RandomShuffle())\n",
    "for i = 1:100\n",
    "    push!(surros, copy(generator()))\n",
    "end\n",
    "\n",
    "generator = surrogenerator(x, RandomShuffle())\n",
    "for i = 1:100\n",
    "    push!(surros, copy(generator()))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Dates\n",
    "\n",
    "DateTime(2018, 04, 08, 01, 00, 00) > Date(2018, 04, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(12)\n",
    "\n",
    "surros2 = []\n",
    "generator = surrogenerator(x, RandomShuffle())\n",
    "for i = 1:100\n",
    "    push!(surros2, copy(generator()))\n",
    "end\n",
    "\n",
    "generator = surrogenerator(x, RandomShuffle())\n",
    "for i = 1:100\n",
    "    push!(surros2, copy(generator()))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1, _ = IP_scores(by_partition=true, actor_number=\"all\")\n",
    "f2, _ = all_users(by_partition=true)\n",
    "\n",
    "# df1 = f1(df)\n",
    "df2 = f2(df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique(df2.username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[:, Not([\"I_score\", \"P_score\", \"retweet_from\"])] == df2[:, Not([\"tweet_count\", \"retweet_from\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1, _ = IP_scores(by_partition=true, min_tweets=3)\n",
    "df1 = f1(df)\n",
    "df1 = df1[:, Not(\"retweet_from\")]\n",
    "df1 = select(df1, sort(names(df1)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine(groupby(df1, \"partition\"), \"username\" => (x->length(unique(x))) => \"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2, _ = IP_scores(by_partition=false, min_tweets=3)\n",
    "\n",
    "dfs = []\n",
    "for partition in unique(df.partition)\n",
    "    push!(dfs, f2(df[df.partition .== partition, :]))\n",
    "end\n",
    "df2 = vcat(dfs...)[:, Not(\"retweet_from\")]\n",
    "df2 = select(df2, sort(names(df2)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 == df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools\n",
    "using CausalityTools\n",
    "\n",
    "igg = InfluenceGraphGenerator(Engine.JointDistanceDistribution, surrogate=nothing)\n",
    "\n",
    "f = (x,y) -> pvalue(jdd(OneSampleTTest, x, y, B=10, D=5, τ=1, μ0=0.0), tail=:right) < 0.001 ? 1 : 0\n",
    "f2 = (x,y) -> igg.causal_function(x,y)\n",
    "\n",
    "x = rand(600)\n",
    "y = rand(600)\n",
    "\n",
    "@btime f2(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = r\"^AGG[0-9]+:\"\n",
    "a = \"AGG1678934: joeoepzocnjocz    dveonoze^12\"\n",
    "if occursin(regex, a)\n",
    "    print(\"cool\")\n",
    "else\n",
    "    print(\"prout\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = transform(groupby(df1, \"username\"), \"effective_category\" => (x -> sum(x .== \"tweet\")) => \"tweet_count\")\n",
    "df2 = transform(groupby(df2, \"username\"), \"effective_category\" => (x -> sum(x .== \"tweet\")) => \"tweet_count\")\n",
    "df3 = transform(groupby(df3, \"username\"), \"effective_category\" => (x -> sum(x .== \"tweet\")) => \"tweet_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1[df1.tweet_count .>= 3, :]\n",
    "df2 = df2[df2.tweet_count .>= 3, :]\n",
    "df3 = df3[df3.tweet_count .>= 3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length(unique(df2.username))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_action_frequency(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length(unique(df2.username))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_count = combine(groupby(tweeters, \"username\"), \"created_at\" => length => \"count\")\n",
    "tweet_count = tweet_count[tweet_count.count .>= 3, :]\n",
    "nodes = tweet_count.username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"GretaThunberg\" in nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Dates\n",
    "\n",
    "df = load_dataset(COP26)\n",
    "\n",
    "df = df[.~ismissing.(df.\"domain\"), :]\n",
    "if eltype(df.\"created_at\") == String\n",
    "    to_datetime = x -> DateTime(split(x, '.')[1], \"yyyy-mm-ddTHH:MM:SS\")\n",
    "    df.\"created_at\" = to_datetime.(df.\"created_at\")\n",
    "end\n",
    "\n",
    "df = PreProcessing.cop_26_dates(df)\n",
    "df = PreProcessing.trust_score(df)\n",
    "df = transform(groupby(df, \"username\"), \"created_at\" => length => \"count\")\n",
    "\n",
    "tweeters = df[df.effective_category .== \"tweet\", :]\n",
    "retweeters = df[df.effective_category .== \"retweet\", :]\n",
    "\n",
    "bad_df = df[df.action .== \"U\", :]\n",
    "good_df = df[df.action .== \"T\", :]\n",
    "\n",
    "weights, u, v, nodes = PreProcessing.IP_graph(df, min_tweets=3)\n",
    "\n",
    "I, P, residuals = PreProcessing.IP_scores(u, v)\n",
    "\n",
    "# Sort in the order of most influence\n",
    "sorting = sortperm(I, rev=true)\n",
    "nodes = nodes[sorting]\n",
    "I = I[sorting]\n",
    "P = P[sorting]\n",
    "\n",
    "# Extract dataframes of 500 most influentials according to I score\n",
    "isin = (x,y) -> x in y\n",
    "influentials = tweeters[isin.(tweeters.username, Ref(nodes[1:500])), :]\n",
    "foo = combine(groupby(influentials, \"username\"), \"action\" => (x -> sum(x .== \"U\")) => \"U_count\")\n",
    "\n",
    "bad_users = foo.username[foo.U_count .> 0]\n",
    "bad_users_rank = [findall(user .== nodes) for user in bad_users];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_bad[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I_normal = I\n",
    "# P_normal = P\n",
    "# nodes_normal = nodes\n",
    "\n",
    "test_normal = nodes_normal[I_normal .> 0];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I_bad = I\n",
    "# P_bad = P\n",
    "# nodes_bad = nodes\n",
    "\n",
    "test_bad = nodes_bad[I_bad .> 0];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[1, 2], [3, 4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = @view a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo2 = a[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[1][1] = 2\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isin = (x,y) -> x in y\n",
    "# sum(isin.(nodes_bad[1:500], Ref(nodes_normal[1:500])))\n",
    "\n",
    "sum(isin.(test_bad, Ref(test_normal))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(I_normal .> 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(12200 - 500) / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting = sortperm(P, rev=true)\n",
    "nodes = nodes[sorting]\n",
    "I = I[sorting]\n",
    "P = P[sorting]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I0 = I .== 0\n",
    "P0 = P .== 0\n",
    "\n",
    "I1 = I .!= 0\n",
    "P1 = P .!= 0;\n",
    "\n",
    "# sort(I0) == sort(P1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(I1 .&& P0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(I1 .|| P1) / length(I0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tweeters[tweeters.action .== \"U\", :]\n",
    "test = transform(groupby(test, \"username\"), \"created_at\" => length => \"Ucount\")\n",
    "\n",
    "length(unique(test[test.Ucount .>= 2, \"username\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes[123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length(residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ = residuals\n",
    "plt.figure()\n",
    "plt.plot(1:length(res_), res_)\n",
    "plt.yscale(\"log\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_actors = unique(test[ismissing.(test.rt_from), \"username\"])\n",
    "counter = []\n",
    "\n",
    "rts = test[.!ismissing.(test.rt_from), :]\n",
    "# mapping = countmap(rts)\n",
    "# transform(groupby(df, \"username\"), \"created_at\" => length => \"tweet_count\")\n",
    "\n",
    "for (i, a) in enumerate(initial_actors)\n",
    "    indices = findall(rts.rt_from .== a)\n",
    "    push!(counter, rts.username[indices])\n",
    "    # counter[i] = sum(rts.rt_from .== a)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = [length(i) for i in counter]\n",
    "sorting = sortperm(N, rev=true)\n",
    "N = N[sorting]\n",
    "initial_actors = initial_actors[sorting]\n",
    "counter = counter[sorting]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_actors[1:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_actors = unique(test[ismissing.(test.rt_from), \"username\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools\n",
    "\n",
    "counter = []\n",
    "\n",
    "@btime begin\n",
    "    indices = findall(rts.rt_from .== initial_actors[1])\n",
    "    push!(counter, rts.username[indices])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# foo = test[test.category .== Ref([\"tweeted\"]), :]\n",
    "# foo = transform(groupby(foo, \"username\"), \"created_at\" => length => \"tweet_count\")\n",
    "# foo = foo[foo.tweet_count .>= 1, :]\n",
    "# length(unique(foo.username))\n",
    "\n",
    "\n",
    "initial_actors = unique(test[.ismissing.(test.rt_from), \"username\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(.!ismissing.(test.rt_from))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = test[startswith.(test.text, Ref(\"RT @\")) .&& test.category .!= Ref([\"retweeted\"]), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(foo, allcols=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_user(test.text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length(unique(test.username))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(test.category .== Ref([\"quoted\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "followers[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(followers .== 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "261259/50\n",
    "\n",
    "## DECREASE BINS LOGARITHMICALLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "25929^2/520^2*10/60/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(0 .< followers .< 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(followers .== 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(followers .> 500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = collect(1:10)\n",
    "sorting = sortperm(test, rev=true)\n",
    "test[sorting]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "76d3d9ea52ae0f582c7ab8c3bbc7d312c08f3afa3631e827e583225c1557aa69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
